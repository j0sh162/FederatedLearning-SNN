
#FL Settings
fl:
  num_rounds: 10
  num_clients: 2
  batch_size: 20
  non_iid: False # Set to false for iid; Set to true for non-iid
  differential_privacy: False
  noise_multiplier: 0.05
  clipping_norm: 0.3 # Recommendation by Flower
  num_sampled_clients: 20


#Client settings - what server tell each client to use in their local training
client:
  lr: 1.0 # learning rate
  betas: [0.9, 0.999] # betas for Adam optimizer
  momentum: 0.9
  local_epochs: 10
  num_classes: 10 # (N)MNIST data has 10 classes
  num_clients_per_round_fit: 2
  num_clients_per_round_evaluate: 2 #After each round if we have 100 clients 1/4 will be used to evaluate what's characteristic of the global model

#Dataset selection
dataset:
  name: NMNIST  # Change to "cifar10" to switch dataset

# Dataset-specific configs
datasets:
  mnist:
    path: ./data/mnist
    input_size: 28
    channels: 1
    num_classes: 10
    batch_size: 32
  cifar10:
    path: ./data/cifar10
    input_size: 32
    channels: 3
    num_classes: 10
    batch_size: 64
  NMNIST:
    path: ./data
    input_size: 34
    channels: 2
    num_classes: 10
    batch_size: 128 # was 50 # for testing

defaults: #Here we define which model and strategy we wish to use,
  - strategy: fedavg #create a config in conf/strategy and define the strategy parameters
  - model: EVENTPROP #create a model yaml in conf/model and define the input parameters for that model

